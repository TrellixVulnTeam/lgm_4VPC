{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"GPU setup\"\"\"\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Imports, define RBM model\"\"\"\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils.data import tfr_dataset_eager, parse_img_label_tfr\n",
    "from utils.rbm import repeated_gibbs, gibbs_update_brbm, energy_rbm\n",
    "from utils.viz import random_sample_grid, img_grid_npy, imshow\n",
    "\n",
    "\n",
    "# data\n",
    "batch_size = 256\n",
    "train_steps = 1500\n",
    "\n",
    "parse_fn = lambda x: parse_img_label_tfr(x, (32*32,))\n",
    "data = tfr_dataset_eager(\"/cache/tfrs/mnist_train.tfr\", batch_size, parse_fn,\n",
    "                         shufrep=60000)\n",
    "\n",
    "\n",
    "# model\n",
    "mode = \"pcd\"\n",
    "if mode not in [\"pcd\", \"cd\", \"naive\"]:\n",
    "    raise ValueError(\"uh oh!\")\n",
    "\n",
    "n_h = 1000\n",
    "w_vh = tf.Variable(tf.random.uniform([32*32, n_h], -0.1, 0.1))\n",
    "b_v = tf.Variable(tf.zeros([32*32]))\n",
    "b_h = tf.Variable(tf.zeros([n_h]))\n",
    "weights = [w_vh, b_v, b_h]\n",
    "\n",
    "# compute marginal for better sampling\n",
    "data_once = tfr_dataset_eager(\"/cache/tfrs/mnist_train.tfr\", 60000, parse_fn)\n",
    "# 1 is the batch axis; 0 is an extra axis we get for the list wrapper (unnecessary)\n",
    "marginals = tf.reduce_mean([img for (img, _) in data_once], axis=[0, 1])\n",
    "\n",
    "\n",
    "start_sampler = tfp.distributions.Bernoulli(probs=0.5, dtype=tf.float32)\n",
    "marginal_sampler = tfp.distributions.Bernoulli(probs=marginals, dtype=tf.float32)\n",
    "\n",
    "#opt = tf.optimizers.SGD(0.1)\n",
    "opt = tf.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train\"\"\"\n",
    "\n",
    "@tf.function\n",
    "def train(batch, v_sampled=None, h_sampled=None):\n",
    "    \"\"\"v_sampled and h_sampled are used only for PCD.\n",
    "    \n",
    "    It's always passed because I'm lazy.\n",
    "    \"\"\"\n",
    "    v_data = batch\n",
    "    # a bit of a cheat: we \"should\" also be sampling h here, like so:\n",
    "    #_, h_data = gibbs_update_brbm((v_data, h_random), w_vh, b_v, b_h)\n",
    "    # where h_random is a dummy.\n",
    "    h_data = tf.nn.sigmoid(tf.matmul(v_data, w_vh) + b_h)\n",
    "    \n",
    "    if mode == \"cd\":\n",
    "        v_sampled, h_sampled = repeated_gibbs(\n",
    "            (v_data, h_data), 20, gibbs_update_brbm,\n",
    "            w_vh=w_vh, b_v=b_v, b_h=b_h)\n",
    "    elif mode == \"pcd\":\n",
    "        v_sampled, h_sampled = repeated_gibbs(\n",
    "            (v_sampled, h_sampled), 20, gibbs_update_brbm,\n",
    "            w_vh=w_vh, b_v=b_v, b_h=b_h)\n",
    "    else:\n",
    "        v_random = marginal_sampler.sample(tf.shape(batch)[0])\n",
    "        # this is just a dummy\n",
    "        h_random = start_sampler.sample([tf.shape(batch)[0], n_h])\n",
    "        v_sampled, h_sampled = repeated_gibbs(\n",
    "            (v_random, h_random), 200, gibbs_update_brbm,\n",
    "            w_vh=w_vh, b_v=b_v, b_h=b_h)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits_pos = tf.reduce_mean(-energy_rbm(v_data, h_data, w_vh, b_v, b_h))\n",
    "        logits_neg = tf.reduce_mean(\n",
    "            -energy_rbm(v_sampled, h_sampled, w_vh, b_v, b_h))\n",
    "        loss = -(logits_pos - logits_neg)\n",
    "    grads = tape.gradient(loss, weights)\n",
    "    opt.apply_gradients(zip(grads, weights))\n",
    "    \n",
    "    return loss, v_sampled, h_sampled\n",
    "\n",
    "\n",
    "v_samp = marginal_sampler.sample(batch_size)\n",
    "h_samp = start_sampler.sample([batch_size, n_h])\n",
    "for step, (img_batch, _) in enumerate(data):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    loss, v_samp, h_samp = train(img_batch, v_samp, h_samp)\n",
    "    if not step % 50:\n",
    "        print(\"Step\", step)\n",
    "        print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"weights...\")\n",
    "for img in w_vh.numpy().T:\n",
    "    absmax = np.max(np.abs(img))\n",
    "    plt.imshow(img.reshape((32,32)), vmin=-absmax, vmax=absmax, cmap=\"RdBu_r\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"random samples\"\"\"\n",
    "v_random = marginal_sampler.sample([49])\n",
    "h_random = start_sampler.sample([49, n_h])\n",
    "img_sample, _ = repeated_gibbs(\n",
    "        (v_random, h_random), 200, gibbs_update_brbm,\n",
    "        w_vh=w_vh, b_v=b_v, b_h=b_h)\n",
    "\n",
    "img_sample = [img.numpy().reshape((32, 32)) for img in img_sample]\n",
    "grid = img_grid_npy(img_sample, 7, 7, normalize=False)\n",
    "imshow(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
