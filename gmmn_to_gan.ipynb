{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"GPU setup\"\"\"\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Imports, define GMMN\"\"\"\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils.data import mnist_eager\n",
    "from utils.math import compute_mmd\n",
    "from utils.models import gen_conv_mnist, gen_fc_mnist, enc_fc_mnist, enc_conv_mnist\n",
    "from utils.viz import random_sample_grid\n",
    "\n",
    "\n",
    "# data\n",
    "batch_size = 256\n",
    "train_steps = 1500\n",
    "dim_noise = 100\n",
    "\n",
    "data = mnist_eager(\"data/mnist_train\", batch_size)\n",
    "\n",
    "\n",
    "def noise_fn(n_samples): return tf.random_uniform([n_samples, dim_noise], minval=-1, maxval=1)\n",
    "\n",
    "\n",
    "conv = False\n",
    "if conv:\n",
    "    model = gen_conv_mnist(use_bn=True)\n",
    "else:\n",
    "    model = gen_fc_mnist(use_bn=True)\n",
    "\n",
    "\n",
    "def loss(real, generated):\n",
    "    return tf.sqrt(compute_mmd(real, generated, [0.01, 0.03, 0.1, 0.3, 1.]))\n",
    "\n",
    "\n",
    "opt = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train\"\"\"\n",
    "tf.keras.backend.set_learning_phase(1)\n",
    "for step, (img_batch, _) in enumerate(data):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        gen_batch = model(noise_fn(tf.shape(img_batch)[0]))\n",
    "        mmd = loss(img_batch, gen_batch)\n",
    "    grads = tape.gradient(mmd, model.trainable_variables)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    \n",
    "    if not step % 50:\n",
    "        print(\"Step\", step)\n",
    "        print(\"Loss\", mmd)\n",
    "\n",
    "tf.keras.backend.set_learning_phase(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate samples\"\"\"\n",
    "grid = random_sample_grid(model, noise_fn, grid_dims=(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Define GAN\"\"\"\n",
    "if conv:\n",
    "    discriminator = enc_conv_mnist(1, use_bn=True)\n",
    "else:\n",
    "    discriminator = enc_fc_mnist(1, use_bn=True)\n",
    "generator = model\n",
    "label_smoothing = 0.9\n",
    "\n",
    "\n",
    "def partial_loss(logits, lbls):\n",
    "    return tf.losses.sigmoid_cross_entropy(multi_class_labels=lbls, logits=logits)\n",
    "\n",
    "\n",
    "gen_opt = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)\n",
    "disc_opt = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Train GAN\"\"\"\n",
    "tf.keras.backend.set_learning_phase(1)\n",
    "for step, (img_batch, _) in enumerate(data):\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    # prepare mixed batch for discriminator training\n",
    "    # For batchnorm to work better, we feed only real images, then only \n",
    "    # generated ones and then average the gradients\n",
    "    batch_dim = tf.shape(img_batch)[0]\n",
    "    gen_batch = generator(noise_fn(batch_dim))\n",
    "    #full_batch = tf.concat([img_batch, gen_batch], axis=0)\n",
    "    #full_labels = tf.concat([0.9*tf.ones([batch_dim, 1]), tf.zeros([batch_dim, 1])], axis=0)\n",
    "    real_labels = label_smoothing*tf.ones([batch_dim, 1])\n",
    "    gen_labels = tf.zeros([batch_dim, 1])\n",
    "    with tf.GradientTape() as d_tape:\n",
    "        d_loss_real = partial_loss(discriminator(img_batch), real_labels)\n",
    "        d_loss_fake = partial_loss(discriminator(gen_batch), gen_labels)\n",
    "        d_loss = 0.5 * (d_loss_real + d_loss_fake)\n",
    "    d_grads = d_tape.gradient(d_loss, discriminator.trainable_variables)\n",
    "    disc_opt.apply_gradients(zip(d_grads, discriminator.trainable_variables))\n",
    "    \n",
    "    # fresh generated batch for generator training\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as g_tape:\n",
    "        for vari in generator.trainable_variables:\n",
    "            g_tape.watch(vari)\n",
    "        gen_only_batch = generator(noise_fn(2*batch_dim))\n",
    "        g_loss = partial_loss(discriminator(gen_only_batch),\n",
    "                              label_smoothing*tf.ones([2*batch_dim, 1]))\n",
    "    g_grads = g_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    gen_opt.apply_gradients(zip(g_grads, generator.trainable_variables))\n",
    "    \n",
    "    if not step % 50:\n",
    "        print(\"Step\", step)\n",
    "        print(\"Gen Loss\", g_loss)\n",
    "        print(\"Disc Loss\", d_loss)\n",
    "\n",
    "tf.keras.backend.set_learning_phase(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate samples\"\"\"\n",
    "grid = random_sample_grid(generator, noise_fn, grid_dims=(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
