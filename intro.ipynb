{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only run if on a multi-GPU setup\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils.data import tfr_dataset_eager, parse_img_label_tfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can easily switch to other datasets by changing the tfrecord path\n",
    "# make sure to use the correct shape in the parse function\n",
    "# e.g. 32,32,3 for color images and CNNs.\n",
    "parse_fn = lambda x: parse_img_label_tfr(x, (1024,))\n",
    "mnist_data = tfr_dataset_eager([\"/cache/tfrs/mnist_train.tfr\"], 256, parse_fn, shufrep=60000)\n",
    "mnist_test = tfr_dataset_eager([\"/cache/tfrs/mnist_test.tfr\"], 1000, parse_fn)\n",
    "\n",
    "# if you wanna try SVHN you need this because for some reason the digit 0 has index 10. this replaces it by 0.\n",
    "#mnist_data = mnist_data.map(lambda x, lbls: (x, tf.where(tf.equal(lbls, 10), tf.zeros(tf.shape(lbls), tf.int32), lbls)))\n",
    "#mnist_test = mnist_test.map(lambda x, lbls: (x, tf.where(tf.equal(lbls, 10), tf.zeros(tf.shape(lbls), tf.int32), lbls)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at some data points\n",
    "for img_batch, lbl_batch in mnist_data:\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.imshow(np.reshape(img_batch[0], [32, 32]), cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "    print(lbl_batch[0])\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic linear model. also displays tensorboard usage\n",
    "w = tf.Variable(tf.random.uniform([1024, 10], -.1, .1))\n",
    "b = tf.Variable(tf.zeros(10))\n",
    "loss_fn = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "opt = tf.optimizers.Adam()\n",
    "\n",
    "with tf.summary.create_file_writer(\"/cache/tensorboard-logdir/linear\").as_default():\n",
    "    for step, (img_batch, lbl_batch) in enumerate(mnist_data):\n",
    "        if step > 1500:\n",
    "            break\n",
    "        if not step % 100:\n",
    "            print(\"Step\", step)\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = tf.matmul(img_batch, w) + b\n",
    "            xent = loss_fn(lbl_batch, logits)\n",
    "        grads = tape.gradient(xent, [w, b])\n",
    "        opt.apply_gradients(zip(grads, [w, b]))\n",
    "        tf.summary.scalar(\"cross entropy\", xent, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras metrics for easy evaluation etc. (could also include these during training!)\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "for (img_batch, lbl_batch) in mnist_test:\n",
    "    logits = tf.matmul(img_batch, w) + b\n",
    "    val_acc_metric(lbl_batch, logits)\n",
    "val_acc = val_acc_metric.result()\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try CIFAR10, CNN, with keras.Sequential\n",
    "parse_fn = lambda x: parse_img_label_tfr(x, (32, 32, 3))\n",
    "cifar_data = tfr_dataset_eager([\"/cache/tfrs/cifar10_train.tfr\"], 256, parse_fn, shufrep=60000)\n",
    "cifar_test = tfr_dataset_eager([\"/cache/tfrs/cifar10_test.tfr\"], 1000, parse_fn)\n",
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Conv2D(16, 3, activation=tf.nn.relu, padding=\"same\"),\n",
    "                             tf.keras.layers.MaxPooling2D(padding=\"same\"),\n",
    "                             tf.keras.layers.Conv2D(32, 3, activation=tf.nn.relu, padding=\"same\"),\n",
    "                             tf.keras.layers.MaxPooling2D(padding=\"same\"),\n",
    "                             tf.keras.layers.Conv2D(64, 3, activation=tf.nn.relu, padding=\"same\"),\n",
    "                             tf.keras.layers.MaxPooling2D(padding=\"same\"),\n",
    "                             tf.keras.layers.Flatten(),\n",
    "                             tf.keras.layers.Dense(10)])\n",
    "\n",
    "loss_fn = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "opt = tf.optimizers.Adam()\n",
    "\n",
    "# optional to build now, but only after building do we have variables available\n",
    "model.build((None, 32, 32, 3))\n",
    "varis = model.trainable_variables\n",
    "\n",
    "\n",
    "# this decorator activates graph mode for this function (and everything it calls)\n",
    "# try removing it and see how many smiley faces are printed in each case\n",
    "@tf.function\n",
    "def train(imgs, lbls):\n",
    "    print(\":-)\")\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(imgs)\n",
    "        xent = loss_fn(lbls, logits)\n",
    "    grads = tape.gradient(xent, varis)\n",
    "    opt.apply_gradients(zip(grads, varis))\n",
    "    \n",
    "    return xent\n",
    "\n",
    "\n",
    "# in principle the whole training loop should be wrappable in a tf.function\n",
    "# but I haven't figured it out yet :p\n",
    "for step, (img_batch, lbl_batch) in enumerate(cifar_data):\n",
    "    if step > 5000:\n",
    "        break\n",
    "        \n",
    "    xent = train(img_batch, lbl_batch)\n",
    "        \n",
    "    if not step % 100:\n",
    "        print(\"Step\", step)\n",
    "        print(\"loss\", xent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing keras metric to manual computation\n",
    "# note that this is eager again\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for (img_batch, lbl_batch) in cifar_test:\n",
    "    logits = model(img_batch)\n",
    "    val_acc_metric(lbl_batch, logits)\n",
    "    \n",
    "    preds = logits.numpy().argmax(axis=-1)\n",
    "    total += len(lbl_batch.numpy())\n",
    "    correct += sum(preds == lbl_batch.numpy())\n",
    "\n",
    "\n",
    "print(val_acc_metric.result())\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# broken example. bonus assignment: make it work (on the GPU) ;-)\n",
    "from tensorflow.python.ops import control_flow_util\n",
    "control_flow_util.ENABLE_CONTROL_FLOW_V2 = True\n",
    "\n",
    "model = tf.keras.Sequential([tf.keras.layers.Conv2D(16, 3, activation=tf.nn.relu, padding=\"same\"),\n",
    "                             tf.keras.layers.MaxPooling2D(padding=\"same\"),\n",
    "                             tf.keras.layers.Conv2D(32, 3, activation=tf.nn.relu, padding=\"same\"),\n",
    "                             tf.keras.layers.MaxPooling2D(padding=\"same\"),\n",
    "                             tf.keras.layers.Conv2D(64, 3, activation=tf.nn.relu, padding=\"same\"),\n",
    "                             tf.keras.layers.MaxPooling2D(padding=\"same\"),\n",
    "                             tf.keras.layers.Flatten(),\n",
    "                             tf.keras.layers.Dense(10)])\n",
    "\n",
    "loss_fn = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "opt = tf.optimizers.Adam()\n",
    "\n",
    "model.build((None, 32, 32, 3))\n",
    "varis = model.trainable_variables\n",
    "\n",
    "\n",
    "def train_step(imgs, lbls):\n",
    "    lbls = tf.where(tf.equal(lbls, 10), tf.zeros(tf.shape(lbls), tf.int32), lbls)\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(imgs)\n",
    "        xent = loss_fn(lbls, logits)\n",
    "    grads = tape.gradient(xent, varis)\n",
    "    opt.apply_gradients(zip(grads, varis))\n",
    "    \n",
    "    return xent\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_loop():\n",
    "    step = tf.constant(0, dtype=tf.int32)\n",
    "    \n",
    "    for img_batch, lbl_batch in cifar1500:\n",
    "        xent = train_step(img_batch, lbl_batch)\n",
    "        step += 1\n",
    "        \n",
    "        if tf.equal(step % 100, 0):\n",
    "            tf.print(\"Step\", step)\n",
    "            tf.print(\"loss\", xent)\n",
    "    \n",
    "\n",
    "cifar1500 = cifar_data.take(1500)\n",
    "train_loop()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate yet again\n",
    "val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "for (img_batch, lbl_batch) in cifar_test:\n",
    "    logits = model(img_batch)\n",
    "    val_acc_metric(lbl_batch, logits)\n",
    "val_acc = val_acc_metric.result()\n",
    "print(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the full keras capacities for easy training (but harder to customize)\n",
    "model = tf.keras.Sequential([tf.keras.layers.Conv2D(16, 3, activation=tf.nn.relu, padding=\"same\"),\n",
    "                             tf.keras.layers.MaxPooling2D(padding=\"same\"),\n",
    "                             tf.keras.layers.Conv2D(32, 3, activation=tf.nn.relu, padding=\"same\"),\n",
    "                             tf.keras.layers.MaxPooling2D(padding=\"same\"),\n",
    "                             tf.keras.layers.Conv2D(64, 3, activation=tf.nn.relu, padding=\"same\"),\n",
    "                             tf.keras.layers.MaxPooling2D(padding=\"same\"),\n",
    "                             tf.keras.layers.Flatten(),\n",
    "                             tf.keras.layers.Dense(10)])\n",
    "\n",
    "model.compile(optimizer=tf.optimizers.Adam(),\n",
    "              loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "model.fit(cifar_data, epochs=10, steps_per_epoch=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation is easy! uses the metrics specified in compile + loss\n",
    "model.evaluate(cifar_test)\n",
    "# predict on whole test set at once, returns a numpy array with logits\n",
    "preds = model.predict(cifar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can still use the model as callable!\n",
    "model(tf.random.normal([10, 32, 32, 3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
