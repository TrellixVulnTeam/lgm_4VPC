{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"GPU setup\"\"\"\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Imports, define GAN\"\"\"\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils.data import tfr_dataset_eager, parse_img_label_tfr\n",
    "from utils.models import gen_conv_mnist, gen_fc_mnist, enc_conv_mnist, enc_fc_mnist\n",
    "from utils.viz import random_sample_grid, interpolate, imshow, img_grid_npy\n",
    "\n",
    "\n",
    "# data\n",
    "batch_size = 128  # this is a \"half batch\"!\n",
    "train_steps = 5000\n",
    "dim_code = 64\n",
    "label_smoothing = 0.9\n",
    "coeff = 10.\n",
    "\n",
    "strain_files = [\"/cache/tfrs/svhn_train.tfr\"]\n",
    "stest_files = [\"/cache/tfrs/svhn_test.tfr\"]\n",
    "sparse_fn = lambda x: parse_img_label_tfr(x, (32, 32, 3))\n",
    "\n",
    "mtrain_files = [\"/cache/tfrs/mnist_train.tfr\"]\n",
    "mtest_files = [\"/cache/tfrs/mnist_test.tfr\"]\n",
    "mparse_fn = lambda x: parse_img_label_tfr(x, (32, 32, 1))\n",
    "\n",
    "mnist_data = tfr_dataset_eager(mtrain_files, batch_size, mparse_fn, shufrep=60000)\n",
    "svhn_data = tfr_dataset_eager(strain_files, batch_size, sparse_fn, shufrep=60000)\n",
    "\n",
    "data = tf.data.Dataset.zip((mnist_data, svhn_data))\n",
    "\n",
    "\n",
    "conv = True\n",
    "if conv:\n",
    "    m_to_c = enc_conv_mnist(dim_code, use_bn=True)\n",
    "    c_to_s = gen_conv_mnist(use_bn=True, channels=3)\n",
    "    discriminator_s = enc_conv_mnist(1, use_bn=True)\n",
    "\n",
    "    s_to_c = enc_conv_mnist(dim_code, use_bn=True)\n",
    "    c_to_m = gen_conv_mnist(use_bn=True, channels=1)\n",
    "    discriminator_m = enc_conv_mnist(1, use_bn=True)\n",
    "else:\n",
    "    m_to_c = enc_fc_mnist(dim_code, use_bn=True)\n",
    "    c_to_s = gen_fc_mnist(use_bn=True, channels=3)\n",
    "    discriminator_s = enc_fc_mnist(1, use_bn=True)\n",
    "\n",
    "    s_to_c = enc_fc_mnist(dim_code, use_bn=True)\n",
    "    c_to_m = gen_fc_mnist(use_bn=True, channels=1)\n",
    "    discriminator_m = enc_fc_mnist(1, use_bn=True)\n",
    "m_to_s = lambda x: c_to_s(m_to_c(x))\n",
    "s_to_m = lambda x: c_to_m(s_to_c(x))\n",
    "\n",
    "\n",
    "loss = tf.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "gen_opt = tf.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "disc_opt = tf.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_d(m_batch, s_batch):\n",
    "    # prepare mixed batch for discriminator training\n",
    "    # For batchnorm to work better, we feed only real images, then only \n",
    "    # generated ones and then average the gradients\n",
    "    batch_dim_m = tf.shape(m_batch)[0]\n",
    "    batch_dim_s = tf.shape(s_batch)[0]\n",
    "    \n",
    "    gen_m_batch = s_to_m(s_batch)\n",
    "    gen_s_batch = m_to_s(m_batch)\n",
    "    \n",
    "    real_labels_m = label_smoothing*tf.ones([batch_dim_m, 1])\n",
    "    gen_labels_m = tf.zeros([batch_dim_s, 1])\n",
    "    real_labels_s = label_smoothing*tf.ones([batch_dim_s, 1])\n",
    "    gen_labels_s = tf.zeros([batch_dim_m, 1])\n",
    "    \n",
    "    with tf.GradientTape() as d_tape:\n",
    "        d_m_loss_real = loss(real_labels_m, discriminator_m(m_batch))\n",
    "        d_m_loss_fake = loss(gen_labels_m, discriminator_m(gen_m_batch))\n",
    "        d_m_loss = 0.5 * (d_m_loss_real + d_m_loss_fake)\n",
    "        \n",
    "        d_s_loss_real = loss(real_labels_s, discriminator_s(s_batch))\n",
    "        d_s_loss_fake = loss(gen_labels_s, discriminator_s(gen_s_batch))\n",
    "        d_s_loss = 0.5 * (d_s_loss_real + d_s_loss_fake)\n",
    "        \n",
    "        d_loss = d_m_loss + d_s_loss\n",
    "\n",
    "    d_vars = discriminator_m.trainable_variables + discriminator_s.trainable_variables\n",
    "    d_grads = d_tape.gradient(d_loss, d_vars)\n",
    "    disc_opt.apply_gradients(zip(d_grads, d_vars))\n",
    "    \n",
    "    return d_loss\n",
    "    \n",
    "@tf.function\n",
    "def train_g(m_batch, s_batch):\n",
    "    # fresh generated batch for generator training\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as g_tape:\n",
    "        g_vars = m_to_c.trainable_variables + c_to_s.trainable_variables + s_to_c.trainable_variables + c_to_m.trainable_variables\n",
    "        for vari in g_vars:\n",
    "            g_tape.watch(vari)\n",
    "        gen_only_s_batch = m_to_s(m_batch)\n",
    "        gen_only_m_batch = s_to_m(s_batch)\n",
    "        \n",
    "        g_m_loss = loss(label_smoothing*tf.ones([tf.shape(s_batch)[0], 1]), \n",
    "                        discriminator_m(gen_only_m_batch))\n",
    "        g_s_loss = loss(label_smoothing*tf.ones([tf.shape(m_batch)[0], 1]), \n",
    "                        discriminator_s(gen_only_s_batch))\n",
    "        g_loss = g_m_loss + g_s_loss\n",
    "        \n",
    "        back_to_m_batch = s_to_m(gen_only_s_batch)\n",
    "        back_to_s_batch = m_to_s(gen_only_m_batch)\n",
    "        cyc_loss = tf.reduce_mean(tf.abs(m_batch - back_to_m_batch)) + tf.reduce_mean(tf.abs(s_batch - back_to_s_batch))\n",
    "        full_loss = g_loss + coeff * cyc_loss\n",
    "    g_grads = g_tape.gradient(full_loss, g_vars)\n",
    "    gen_opt.apply_gradients(zip(g_grads, g_vars))\n",
    "    \n",
    "    return g_loss, cyc_loss\n",
    "\n",
    "\n",
    "tf.keras.backend.set_learning_phase(1)\n",
    "enum_data = enumerate(data)\n",
    "for step, ((m_batch, _), (s_batch, _)) in enum_data:\n",
    "    if step > train_steps:\n",
    "        break\n",
    "\n",
    "    d_loss = train_d(m_batch, s_batch)\n",
    "    _, ((m_batch2, _), (s_batch2, _)) = next(enum_data)\n",
    "    g_loss, cyc_loss = train_g(m_batch2, s_batch2)\n",
    "    \n",
    "    if not step % 50:\n",
    "        print(\"Step\", step)\n",
    "        print(\"Gen Loss\", g_loss)\n",
    "        print(\"Disc Loss\", d_loss)\n",
    "        print(\"Cycle Loss\", cyc_loss)\n",
    "\n",
    "tf.keras.backend.set_learning_phase(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_learning_phase(0)\n",
    "\n",
    "test_datam = tfr_dataset_eager(mtest_files, 7*7, mparse_fn)\n",
    "test_datas = tfr_dataset_eager(stest_files, 7*7, sparse_fn)\n",
    "test_data = tf.data.Dataset.zip((test_datam, test_datas))\n",
    "\n",
    "for (m_batch, _), (s_batch, _) in test_data:\n",
    "    gen_s = m_to_s(m_batch)\n",
    "    gen_m = s_to_m(s_batch)\n",
    "    back_to_m = s_to_m(gen_s)\n",
    "    back_to_s = m_to_s(gen_m)\n",
    "    \n",
    "    print(\"Input batches\")\n",
    "    grid_s_orig = img_grid_npy(s_batch.numpy(), 7, 7, normalize=False)\n",
    "    imshow(grid_s_orig)\n",
    "    grid_m_orig = img_grid_npy(m_batch.numpy(), 7, 7, normalize=False)\n",
    "    imshow(grid_m_orig)\n",
    "    \n",
    "    print(\"Conversion results\")\n",
    "    grid_m = img_grid_npy(gen_m.numpy(), 7, 7, normalize=False)\n",
    "    imshow(grid_m)\n",
    "    grid_s = img_grid_npy(gen_s.numpy(), 7, 7, normalize=False)\n",
    "    imshow(grid_s)\n",
    "    \n",
    "    print(\"Back to original\")\n",
    "    grid_s_back = img_grid_npy(back_to_s.numpy(), 7, 7, normalize=False)\n",
    "    imshow(grid_s_back)\n",
    "    grid_m_back = img_grid_npy(back_to_m.numpy(), 7, 7, normalize=False)\n",
    "    imshow(grid_m_back)\n",
    "    input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
